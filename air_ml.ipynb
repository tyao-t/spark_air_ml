{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d02df6-eafb-4508-96c3-6091008eae10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AirML\").getOrCreate()\n",
    "\n",
    "csv_directory_path = \"dbfs:/FileStore/\"\n",
    "\n",
    "files = spark.sparkContext.wholeTextFiles(csv_directory_path).keys().collect()\n",
    "\n",
    "csv_files = [file for file in files if file.endswith(\".csv\")]\n",
    "\n",
    "# for file_name in csv_files:\n",
    "#     print(file_name)\n",
    "#     dbutils.fs.rm(file_name)\n",
    "\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"Number of CSV files in the directory: {num_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84efa060-3b7e-4e4b-b5fa-bb12a5ba4fae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28490\n"
     ]
    }
   ],
   "source": [
    "# print(csv_files)\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_files[:1])\n",
    "    # .option(\"mergeSchema\", \"true\") \\\n",
    "print(df.count())\n",
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND\", \"CIG\", \"VIS\", \"TMP\", \"DEW\", \"SLP\")\n",
    "# df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3d337f-4d4d-4782-bd07-346670ab4109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring_index, split, col\n",
    "\n",
    "df = df.withColumn(\"WND_DIR\", split(col(\"WND\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"WND_TYPE\", split(col(\"WND\"), \",\").getItem(2))\n",
    "df = df.withColumn(\"WND_SPD\", split(col(\"WND\"), \",\").getItem(3))\n",
    "df = df.withColumn(\"CEL_H\", split(col(\"CIG\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"VIS_DIST\", split(col(\"VIS\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"VIS_VAR\", split(col(\"VIS\"), \",\").getItem(2))\n",
    "df = df.withColumn(\"AIR_TMP\", split(col(\"TMP\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"DEW_TMP\", split(col(\"DEW\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"PRESSURE\", split(col(\"SLP\"), \",\").getItem(0))\n",
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_TYPE\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"VIS_VAR\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")\n",
    "# df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723ccb84-889f-42e0-b8ec-2bbb86e076ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"AIR_TMP\", \"DEW_TMP\"]\n",
    "categorical_cols = [\"WND_TYPE\", \"VIS_VAR\"]\n",
    "all_cols = numeric_cols + categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d39b969d-4b5a-4d1d-8eb8-0409833cfe26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b544361328c4657a7f4998cb5f35d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca8098a33924492b99acb5561f9c27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# categorical_cols = [\"WND_TYPE\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\") for col in categorical_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_ONE\") for col in categorical_cols]\n",
    "\n",
    "pipeline_stages = indexers + encoders \n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7317b7c-9fee-4111-b640-a794213c3093",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"WND_TYPE\")\n",
    "# df = df.drop(\"VIS_VAR\")\n",
    "df = df.drop(\"WND_TYPE_Index\")\n",
    "# df = df.drop(\"VIS_VAR_Index\")\n",
    "df = df.withColumnRenamed(\"VIS_VAR_ONE\", \"VIS_VAR\").withColumnRenamed(\"WND_TYPE_ONE\", \"WND_TYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f34e829-5c22-40e2-90ee-f839605ef8cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+-------+---------+-------+-----+--------+-------+-------+-------+--------+\n| LONGITUDE|  LATITUDE|ELEVATION|WND_DIR| WND_TYPE|WND_SPD|CEL_H|VIS_DIST|VIS_VAR|AIR_TMP|DEW_TMP|PRESSURE|\n+----------+----------+---------+-------+---------+-------+-----+--------+-------+-------+-------+--------+\n|-1.1833333|60.1333333|     84.0|    999|(2,[],[])|   0000|22000|  055000|      9|  -0040|  -0053|   10019|\n|-1.1833333|60.1333333|     84.0|    999|(2,[],[])|   0000|99999|  055000|      9|  -0045|  -0056|   10021|\n+----------+----------+---------+-------+---------+-------+-----+--------+-------+-------+-------+--------+\nonly showing top 2 rows\n\n28490\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_TYPE\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"VIS_VAR\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")\n",
    "df.show(2)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f444e021-c30e-45e5-bd5a-d60b8bd03340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27360\n+----------+----------+---------+-------+-------------+-------+-----+--------+-------+-------+-------+--------+\n| LONGITUDE|  LATITUDE|ELEVATION|WND_DIR|     WND_TYPE|WND_SPD|CEL_H|VIS_DIST|VIS_VAR|AIR_TMP|DEW_TMP|PRESSURE|\n+----------+----------+---------+-------+-------------+-------+-----+--------+-------+-------+-------+--------+\n|-1.1833333|60.1333333|     84.0|    NaN|    (2,[],[])|   0000|22000|  055000|      9|  -0040|  -0053|   10019|\n|-1.1833333|60.1333333|     84.0|    NaN|    (2,[],[])|   0000|  NaN|  055000|      9|  -0045|  -0056|   10021|\n|-1.1833333|60.1333333|     84.0|    330|(2,[0],[1.0])|   0010|  NaN|  040000|      9|  -0046|  -0056|   10019|\n+----------+----------+---------+-------+-------------+-------+-----+--------+-------+-------+-------+--------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Cannot predict if the response variable is missing\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import when\n",
    "df = df.filter(df.PRESSURE != 99999)\n",
    "\n",
    "df = df.withColumn(\"LONGITUDE\", when(col(\"LONGITUDE\") == 999999, np.nan).otherwise(col(\"LONGITUDE\"))) \\\n",
    "       .withColumn(\"LATITUDE\", when(col(\"LATITUDE\") == 99999, np.nan).otherwise(col(\"LATITUDE\"))) \\\n",
    "       .withColumn(\"ELEVATION\", when(col(\"ELEVATION\") == 9999, np.nan).otherwise(col(\"ELEVATION\"))) \\\n",
    "       .withColumn(\"WND_DIR\", when(col(\"WND_DIR\") == 999, np.nan).otherwise(col(\"WND_DIR\"))) \\\n",
    "       .withColumn(\"WND_SPD\", when(col(\"WND_SPD\") == 9999, np.nan).otherwise(col(\"WND_SPD\"))) \\\n",
    "       .withColumn(\"CEL_H\", when(col(\"CEL_H\") == 99999, np.nan).otherwise(col(\"CEL_H\"))) \\\n",
    "       .withColumn(\"VIS_DIST\", when(col(\"VIS_DIST\") == 999999, np.nan).otherwise(col(\"VIS_DIST\"))) \\\n",
    "       .withColumn(\"AIR_TMP\", when(col(\"AIR_TMP\") == 9999, np.nan).otherwise(col(\"AIR_TMP\"))) \\\n",
    "       .withColumn(\"DEW_TMP\", when(col(\"DEW_TMP\") == 9999, np.nan).otherwise(col(\"DEW_TMP\")))\n",
    "\n",
    "print(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9f85ce-f4fd-49f8-9fd0-b719c8e7fade",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# assembler_numeric = VectorAssembler(inputCols=numeric_cols, outputCol=\"numerical_features\")\n",
    "# assembler_all = VectorAssembler(inputCols=[\"scaled_numerical_features\"] + [\"WND_TYPE\", \"VIS_VAR\"], outputCol=\"features\")\n",
    "# pipeline = Pipeline(stages=[assembler_numeric, assembler_all])\n",
    "# df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a6c56c-e4a6-4a8c-8141-8fded02efd3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=all_cols, labelCol=\"PRESSURE\",\n",
    "                   maxDepth=7, maxIter=1000, stepSize=0.01, subsamplingRate=0.2)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=17)\n",
    "\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"PRESSURE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762b1d9a-1634-4d1b-a3e6-e8311c145549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit\n",
    "\n",
    "def add_index(df):\n",
    "    window_spec = Window.orderBy(lit(1))\n",
    "    \n",
    "    df_with_index = df.withColumn(\"index\", row_number().over(window_spec) - 1)\n",
    "    return df_with_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5748e3b5-a036-4bb3-b800-040bc9f8d4f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_diff(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "    \n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "\n",
    "    joined_df = df1.join(df2, on=\"index\", how=\"inner\")\n",
    "\n",
    "    columns = []\n",
    "    for col_name in df1.columns:\n",
    "        if col_name != \"index\": columns.append(col_name)\n",
    "        \n",
    "    sum_of_squared_diff = joined_df.select(\n",
    "        *[\n",
    "            spark_sum(\n",
    "                when(\n",
    "                    isnan(col(f\"{col_name}\")) | isnan(col(f\"{col_name}_2\")),\n",
    "                    0\n",
    "                ).otherwise(pow(col(f\"{col_name}\") - col(f\"{col_name}_2\"), 2))\n",
    "            ).alias(f\"sum_of_squared_{col_name}\")\n",
    "            for col_name in columns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    total_sum_of_squared_diff = sum_of_squared_diff.select(\n",
    "        sum([spark_sum(col_name)\n",
    "        for col_name in sum_of_squared_diff.columns]).alias(\"total_sum_of_squared_diff\")\n",
    "    )\n",
    "\n",
    "    # total_sum_of_squared_diff.show()\n",
    "    return total_sum_of_squared_diff.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eea1914-c957-4d29-bb6b-aa83ee35dc8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|\n+---+---+-----+---------+------------+--------------------+\n|  1|  1|    1|        8|           1|                 1.0|\n|  2|  1|    2|        7|           2|                 2.0|\n|  2|  1|    3|        6|           3|                 3.0|\n|  4|  2|    4|        5|           5|                 7.0|\n|  4|  2|    5|        4|           7|                11.0|\n|  4|  2|    6|        3|           9|                15.0|\n|  5|  8|    7|        2|          17|                79.0|\n|  8|  8|    8|        1|          25|               143.0|\n|  9|  8|    9|        1|          33|               207.0|\n+---+---+-----+---------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit\n",
    "\n",
    "data = [\n",
    "    (1, 1),\n",
    "    (2, 1),\n",
    "    (2, 1),\n",
    "    (4, 2),\n",
    "    (4, 2),\n",
    "    (4, 2),\n",
    "    (5, 8),\n",
    "    (8, 8),\n",
    "    (9, 8),\n",
    "]\n",
    "columns = [\"A\", \"Y\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "window_spec_index = Window.orderBy(\"A\")\n",
    "df_with_index = df.withColumn(\"index\", row_number().over(window_spec_index))\n",
    "N = df_with_index.count()  \n",
    "df_with_index = df_with_index.withColumn(\n",
    "    \"remaining\",\n",
    "    when((lit(N) - col(\"index\")) == 0, 1).otherwise(lit(N) - col(\"index\"))\n",
    ")\n",
    "\n",
    "df_sorted = df_with_index.orderBy(col(\"A\"))\n",
    "\n",
    "window_spec_prefix = Window.orderBy(\"A\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "# window_spec_suffix = Window.orderBy(\"A\").rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "\n",
    "df_with_sums = df_sorted.withColumn(\"prefix_sum_Y\", spark_sum(col(\"Y\")).over(window_spec_prefix)) \\\n",
    "                        .withColumn(\"prefix_squared_sum_Y\", spark_sum(pow(col(\"Y\"), 2)).over(window_spec_prefix)) \\\n",
    "                        # .withColumn(\"suffix_sum_Y\", spark_sum(col(\"Y\")).over(window_spec_suffix))\n",
    "\n",
    "df_with_sums.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08613a1-d6fc-4fcf-9ca9-a25970f76314",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+-----------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|              RSS|\n+---+---+-----+---------+------------+--------------------+-----------------+\n|  1|  1|    1|        8|           1|                 1.0|             78.0|\n|  2|  1|    2|        7|           2|                 2.0|67.71428571428572|\n|  2|  1|    3|        6|           3|                 3.0|             54.0|\n|  4|  2|    4|        5|           5|                 7.0|43.94999999999999|\n|  4|  2|    5|        4|           7|                11.0|28.19999999999999|\n|  4|  2|    6|        3|           9|                15.0|              1.5|\n|  5|  8|    7|        2|          17|                79.0|37.71428571428572|\n|  8|  8|    8|        1|          25|               143.0|           64.875|\n|  9|  8|    9|        1|          33|               207.0|             86.0|\n+---+---+-----+---------+------------+--------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "tot_prefix_sum_Y = df_with_sums.orderBy(\"prefix_sum_Y\", ascending=False).limit(1).collect()[0][\"prefix_sum_Y\"]\n",
    "\n",
    "tot_prefix_squared_sum_Y = df_with_sums.orderBy(\"prefix_squared_sum_Y\", ascending=False).limit(1).collect()[0][\"prefix_squared_sum_Y\"]\n",
    "\n",
    "# print(tot_prefix_squared_sum_Y)\n",
    "N = df_with_sums.count()  \n",
    "\n",
    "df_final = df_with_sums.withColumn(\n",
    "    \"RSS\",\n",
    "    tot_prefix_squared_sum_Y - \n",
    "    pow(col(\"prefix_sum_Y\"), 2) / col(\"index\") -\n",
    "    pow(tot_prefix_sum_Y-col(\"prefix_sum_Y\"), 2) / col(\"remaining\")\n",
    ")\n",
    "\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c00bf94-9c74-4fe5-8fff-b3d8f64bdb13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+-----------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|              RSS|\n+---+---+-----+---------+------------+--------------------+-----------------+\n|  1|  1|    1|        8|           1|                 1.0|             78.0|\n|  2|  1|    2|        7|           2|                 2.0|         Infinity|\n|  2|  1|    3|        6|           3|                 3.0|             54.0|\n|  4|  2|    4|        5|           5|                 7.0|         Infinity|\n|  4|  2|    5|        4|           7|                11.0|         Infinity|\n|  4|  2|    6|        3|           9|                15.0|              1.5|\n|  5|  8|    7|        2|          17|                79.0|37.71428571428572|\n|  8|  8|    8|        1|          25|               143.0|           64.875|\n|  9|  8|    9|        1|          33|               207.0|             86.0|\n+---+---+-----+---------+------------+--------------------+-----------------+\n\n4 1.5\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"index\")\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit, lead\n",
    "\n",
    "df_final = df_final.withColumn(\"lead_A\", lead(\"A\").over(window_spec))\n",
    "df_final = df_final.withColumn(\n",
    "    \"RSS\",\n",
    "    when((col(\"lead_A\").isNotNull()) & (col(\"lead_A\") <= col(\"A\")), lit(float(\"inf\"))).otherwise(col(\"RSS\"))\n",
    ")\n",
    "\n",
    "df_final = df_final.drop(\"lead_A\")\n",
    "df_final.show()\n",
    "df_final_sorted = df_final.orderBy(col(\"RSS\"))\n",
    "\n",
    "row0 = df_final_sorted.limit(1).collect()[0]\n",
    "A, min_RSS = row0[\"A\"], row0[\"RSS\"]\n",
    "print(A, min_RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9c57af-2ccc-44a0-823e-6f9e30563ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit, lead, isnan\n",
    "\n",
    "def replace_nans(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "\n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "    joined_df = df1.join(df2, on=\"index\")\n",
    "    # joined_df.show()\n",
    "\n",
    "    for column in df1.columns:\n",
    "        if column != \"index\": \n",
    "            joined_df = joined_df.withColumn(\n",
    "                column,\n",
    "                when(~isnan(col(f\"{column}\")), col(f\"{column}\")).otherwise(col(f\"{column}_2\"))\n",
    "            )\n",
    "    \n",
    "    cols_to_drop = [col for col in joined_df.columns if col.endswith('_2')] + ['index']\n",
    "    result_df = joined_df.drop(*cols_to_drop)\n",
    "    return result_df\n",
    "\n",
    "def keep_nans(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "\n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "    joined_df = df1.join(df2, on=\"index\")\n",
    "\n",
    "    for column in df1.columns:\n",
    "        if column != \"index\": \n",
    "            joined_df = joined_df.withColumn(\n",
    "                column,\n",
    "                when(isnan(col(f\"{column}\")), np.nan).otherwise(col(f\"{column}_2\"))\n",
    "            )\n",
    "    \n",
    "    cols_to_drop = [col for col in joined_df.columns if col.endswith('_2')] + ['index']\n",
    "    result_df = joined_df.drop(*cols_to_drop)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb1cc45-f333-4c92-84d5-b5fa637e310d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3212389668875595\n2.2061849831439435\n1.7338799989544897\n1.5844290856932963\n1.5182814975518553\n1.4804414796314695\n1.4548579755726747\n1.4357188318781642\n1.4205174320012321\n1.408009407802187\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, col, split\n",
    "from pyspark.ml.feature import Imputer, PCA\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    (np.nan, 1.0, 0.0, 0.8414709848078965),\n",
    "    (2.0, 4.0, 0.6931471805599453, 0.9092974268256817),\n",
    "    (3.0, 9.0, 1.0986122886681098, np.nan),\n",
    "    (4.0, np.nan, 1.3862943611198906, -0.7568024953079282),\n",
    "    (5.0, 25.0, 1.6094379124341003, np.nan),\n",
    "    (6.0, 36.0, 1.791759469228055, -0.27941549819892586),\n",
    "    (7.0, np.nan, 1.9459101490553132, 0.6569865987187891),\n",
    "    (8.0, 64.0, np.nan, 0.9893582466233818),\n",
    "    (9.0, 81.0, 2.1972245773362196, 0.4121184852417566),\n",
    "    (10.0, 100.0, np.nan, -0.5440211108893698),\n",
    "    (11.0, 121.0, 2.3978952727983707, -0.9999902065507035),\n",
    "    (12.0, 144.0, 2.4849066497880004, -0.5365729180004349),\n",
    "    (13.0, 169.0, np.nan, 0.4201670368266409),\n",
    "    (14.0, 196.0, 2.6390573296152584, 0.9906073556948704),\n",
    "    (np.nan, 225.0, 2.70805020110221, 0.6502878401571168)\n",
    "]\n",
    "\n",
    "columns = [\"a\", \"b\", \"c\", \"d\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "odf = df # Original DF\n",
    "\n",
    "imputer = Imputer(inputCols=columns, outputCols=columns).setStrategy(\"mean\")\n",
    "df = imputer.fit(df).transform(df)\n",
    "\n",
    "df = standardize_df(df)\n",
    "df_standardized_with_nan = keep_nans(odf, df)\n",
    "\n",
    "# Iterations for feature reconstruction\n",
    "for i in range(10):\n",
    "    assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "    df = assembler.transform(df)\n",
    "\n",
    "    pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    model = pca.fit(df)\n",
    "\n",
    "    result = model.transform(df).select(\"features\", \"pcaFeatures\")\n",
    "\n",
    "    pc_matrix = model.pc.toArray().transpose()\n",
    "\n",
    "    pc_vectors = [Vectors.dense(row) for row in pc_matrix]\n",
    "    # pc_matrix is small and can easily fit into memory\n",
    "    # print(pc_matrix.shape, pc_matrix)\n",
    "    def reconstruct_features_udf(pca_features):\n",
    "        reconstructed_features = Vectors.dense(\n",
    "            [sum(pca_feature * pc_vector[i] for pca_feature, pc_vector in zip(pca_features, pc_vectors)) for i in range(len(pc_vectors[0]))]\n",
    "        )\n",
    "        return reconstructed_features\n",
    "\n",
    "    reconstruct_features = udf(reconstruct_features_udf, VectorUDT())\n",
    "\n",
    "    reconstructed_df = result.withColumn(\"reconstructed_features\", reconstruct_features(col(\"pcaFeatures\"))).drop(\"pcaFeatures\").drop(\"features\").withColumnRenamed(\"reconstructed_features\", \"features\")\n",
    "\n",
    "    # reconstructed_df.show(truncate=False)\n",
    "    recon_df = reverse_vec_assemble(reconstructed_df, df.columns)\n",
    "    diff = compute_diff(recon_df, df_standardized_with_nan)\n",
    "    print(diff)\n",
    "    df = replace_nans(df_standardized_with_nan, recon_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3857e5-07fb-45da-94cf-522f1111641a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "\n",
    "def standardize_df(df):\n",
    "    for column in df.columns:\n",
    "        mean_val = df.select(mean(col(column))).first()[0]\n",
    "        stddev_val = df.select(stddev(col(column))).first()[0]\n",
    "        df = df.withColumn(column, (col(column) - mean_val) / stddev_val)\n",
    "    return df\n",
    "\n",
    "# data = [(1, 2), (2, 4), (3, 6)]\n",
    "# columns = [\"col1\", \"col2\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "# df = standardize_df(df)\n",
    "# df.show()\n",
    "\n",
    "def reverse_vec_assemble(df_assembled, columns):\n",
    "    vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "    df_array = df_assembled.withColumn(\"features_array\", vector_to_array(\"features\"))\n",
    "\n",
    "    for i, col_name in enumerate(columns):\n",
    "        df_array = df_array.withColumn(col_name, df_array[\"features_array\"].getItem(i))\n",
    "\n",
    "    df_array = df_array.drop(\"features_array\").drop(\"features\")\n",
    "    return df_array"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "air_ml",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
