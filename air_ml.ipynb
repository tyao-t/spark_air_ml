{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82dffa15-72c7-430f-b2ae-97f48c255546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, col, split\n",
    "from pyspark.ml.feature import Imputer, PCA\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit, lead, isnan\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69dd3d04-6bf2-4460-b0fa-3a7c224a0e80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_directory_path = \"dbfs:/FileStore/\"\n",
    "files = spark.sparkContext.wholeTextFiles(csv_directory_path).keys().collect()\n",
    "csv_files = [file for file in files if file.endswith(\".csv\")]\n",
    "# for file_name in csv_files:\n",
    "#     dbutils.fs.rm(file_name)\n",
    "# %fs rm dbfs:/FileStore/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d02df6-eafb-4508-96c3-6091008eae10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files in the directory: 200\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"AirML\").getOrCreate()\n",
    "\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"Number of CSV files in the directory: {num_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84efa060-3b7e-4e4b-b5fa-bb12a5ba4fae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n+-----------+-----------+---------+--------------+-----------+------------+-------+-------+-------+\n|  LONGITUDE|   LATITUDE|ELEVATION|           WND|        CIG|         VIS|    TMP|    DEW|    SLP|\n+-----------+-----------+---------+--------------+-----------+------------+-------+-------+-------+\n|  69.694708|  40.215394|   441.96|070,1,N,0100,1|22000,1,9,N|009999,1,9,9|+0140,1|+0040,1|99999,9|\n|-40.7333333|-20.6333333|     36.5|999,9,9,9999,9|99999,9,9,9|999999,9,9,9|+9999,9|+9999,9|99999,9|\n|141.5166666| 40.5333333|    28.54|110,1,N,0046,1|99999,9,9,N|020000,1,9,9|+0200,1|+0178,1|10163,1|\n+-----------+-----------+---------+--------------+-----------+------------+-------+-------+-------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# print(csv_files)\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_files)\n",
    "    # .option(\"mergeSchema\", \"true\") \\\n",
    "# print(df.count())\n",
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND\", \"CIG\", \"VIS\", \"TMP\", \"DEW\", \"SLP\")\n",
    "sampled_rows = df.rdd.takeSample(False, 5000, seed=177)\n",
    "df = spark.createDataFrame(sampled_rows, df.schema)\n",
    "print(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3d337f-4d4d-4782-bd07-346670ab4109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring_index, split, col\n",
    "\n",
    "df = df.withColumn(\"WND_DIR\", split(col(\"WND\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"WND_TYPE\", split(col(\"WND\"), \",\").getItem(2))\n",
    "df = df.withColumn(\"WND_SPD\", split(col(\"WND\"), \",\").getItem(3))\n",
    "df = df.withColumn(\"CEL_H\", split(col(\"CIG\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"VIS_DIST\", split(col(\"VIS\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"VIS_VAR\", split(col(\"VIS\"), \",\").getItem(2))\n",
    "df = df.withColumn(\"AIR_TMP\", split(col(\"TMP\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"DEW_TMP\", split(col(\"DEW\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"PRESSURE\", split(col(\"SLP\"), \",\").getItem(0))\n",
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_TYPE\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"VIS_VAR\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")\n",
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723ccb84-889f-42e0-b8ec-2bbb86e076ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"AIR_TMP\", \"DEW_TMP\"]\n",
    "categorical_cols = []\n",
    "# categorical_cols = [\"WND_TYPE\", \"VIS_VAR\"]\n",
    "all_cols = numeric_cols + categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d39b969d-4b5a-4d1d-8eb8-0409833cfe26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:714)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:430)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:430)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1225)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:958)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:914)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:679)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:695)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:759)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:552)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:546)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:546)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:524)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:150)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:147)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:714)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:430)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:430)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1225)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:958)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:914)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:679)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:695)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:759)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:552)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:546)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:546)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:524)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:150)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:147)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# categorical_cols = [\"WND_TYPE\"]\n",
    "# Do not execute the above code unless you want to deal with categorical vars\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\") for col in categorical_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_ONE\") for col in categorical_cols]\n",
    "\n",
    "pipeline_stages = indexers + encoders \n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.drop(\"WND_TYPE\")\n",
    "# df = df.drop(\"VIS_VAR\")\n",
    "df = df.drop(\"WND_TYPE_Index\")\n",
    "# df = df.drop(\"VIS_VAR_Index\")\n",
    "df = df.withColumnRenamed(\"VIS_VAR_ONE\", \"VIS_VAR\").withColumnRenamed(\"WND_TYPE_ONE\", \"WND_TYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f34e829-5c22-40e2-90ee-f839605ef8cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+-------+-------+-----+--------+-------+-------+--------+\n|  LONGITUDE|   LATITUDE|ELEVATION|WND_DIR|WND_SPD|CEL_H|VIS_DIST|AIR_TMP|DEW_TMP|PRESSURE|\n+-----------+-----------+---------+-------+-------+-----+--------+-------+-------+--------+\n|  69.694708|  40.215394|   441.96|    070|   0100|22000|  009999|  +0140|  +0040|   99999|\n|-40.7333333|-20.6333333|     36.5|    999|   9999|99999|  999999|  +9999|  +9999|   99999|\n+-----------+-----------+---------+-------+-------+-----+--------+-------+-------+--------+\nonly showing top 2 rows\n\n5000\n"
     ]
    }
   ],
   "source": [
    "# df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_TYPE\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"VIS_VAR\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")\n",
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")\n",
    "df.show(2)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f444e021-c30e-45e5-bd5a-d60b8bd03340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1766\n+-----------+----------+---------+-------+-------+-----+--------+-------+-------+--------+\n|  LONGITUDE|  LATITUDE|ELEVATION|WND_DIR|WND_SPD|CEL_H|VIS_DIST|AIR_TMP|DEW_TMP|PRESSURE|\n+-----------+----------+---------+-------+-------+-----+--------+-------+-------+--------+\n|141.5166666|40.5333333|    28.54|    110|   0046|  NaN|  020000|  +0200|  +0178|   10163|\n|  4.9666666|53.6166666|     30.0|    240|   0040|00210|  005000|  +0070|  +0053|   10117|\n| 19.0166666|46.1833333|    113.0|    300|   0010|  NaN|     NaN|  +0077|  +0007|   10339|\n+-----------+----------+---------+-------+-------+-----+--------+-------+-------+--------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Cannot predict if the response variable is missing\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\"LONGITUDE\", when(col(\"LONGITUDE\") == 999999, np.nan).otherwise(col(\"LONGITUDE\"))) \\\n",
    "       .withColumn(\"LATITUDE\", when(col(\"LATITUDE\") == 99999, np.nan).otherwise(col(\"LATITUDE\"))) \\\n",
    "       .withColumn(\"ELEVATION\", when(col(\"ELEVATION\") == 9999, np.nan).otherwise(col(\"ELEVATION\"))) \\\n",
    "       .withColumn(\"WND_DIR\", when(col(\"WND_DIR\") == 999, np.nan).otherwise(col(\"WND_DIR\"))) \\\n",
    "       .withColumn(\"WND_SPD\", when(col(\"WND_SPD\") == 9999, np.nan).otherwise(col(\"WND_SPD\"))) \\\n",
    "       .withColumn(\"CEL_H\", when(col(\"CEL_H\") == 99999, np.nan).otherwise(col(\"CEL_H\"))) \\\n",
    "       .withColumn(\"VIS_DIST\", when(col(\"VIS_DIST\") == 999999, np.nan).otherwise(col(\"VIS_DIST\"))) \\\n",
    "       .withColumn(\"AIR_TMP\", when(col(\"AIR_TMP\") == 9999, np.nan).otherwise(col(\"AIR_TMP\"))) \\\n",
    "       .withColumn(\"DEW_TMP\", when(col(\"DEW_TMP\") == 9999, np.nan).otherwise(col(\"DEW_TMP\")))\n",
    "\n",
    "df = df.filter(df.PRESSURE != 99999)\n",
    "\n",
    "print(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de5e464-0fbb-42aa-b64f-18cfa2bccdc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[LONGITUDE: string, LATITUDE: string, ELEVATION: string, WND_DIR: string, WND_SPD: string, CEL_H: string, VIS_DIST: string, AIR_TMP: string, DEW_TMP: string, PRESSURE: string]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.persist()\n",
    "oodf = df\n",
    "oodf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9f85ce-f4fd-49f8-9fd0-b719c8e7fade",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# assembler_numeric = VectorAssembler(inputCols=numeric_cols, outputCol=\"numerical_features\")\n",
    "# assembler_all = VectorAssembler(inputCols=[\"scaled_numerical_features\"] + [\"WND_TYPE\", \"VIS_VAR\"], outputCol=\"features\")\n",
    "# pipeline = Pipeline(stages=[assembler_numeric, assembler_all])\n",
    "# df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a6c56c-e4a6-4a8c-8141-8fded02efd3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1766\n+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------+--------------------+\n|          LONGITUDE|           LATITUDE|          ELEVATION|            WND_DIR|            WND_SPD|             CEL_H|            VIS_DIST|            AIR_TMP|            DEW_TMP|PRESSURE|            features|\n+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------+--------------------+\n|0.07428901043077044|0.47654949215081005| 0.6791390885604672| 0.5123068832964599| 1.4111111307378563|-6.114125302646556|0.057522309086104596|-0.5060710675845688|-0.1414624032607053| 10067.0|[0.07428901043077...|\n|-1.2638035636611855|0.29532851869223686| 2.8312751116269785|-0.8891031335729688|0.12908506112898138| -8.82601070271537| -0.5376254584137213| -1.216483725483591|-1.3959729522551867| 10335.0|[-1.2638035636611...|\n| 1.6444747636944088|-2.4663862112360584|-0.5858355309792764|0.08110380118278952| 0.8769336017341585|-5.102287203391158|   3.473527021550835|0.29964084930090756| 0.5445980532206518| 10116.0|[1.64447476369440...|\n+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------+--------------------+\nonly showing top 3 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:714)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:430)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:430)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1225)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:958)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:914)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:679)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:695)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:759)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:552)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:546)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:546)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:524)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:150)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:147)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:103)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2(SequenceExecutionState.scala:103)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$2$adapted(SequenceExecutionState.scala:100)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:100)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:714)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:430)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:430)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.cancelExecution(ChauffeurState.scala:1225)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:958)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:67)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:67)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:914)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:679)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:695)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:759)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:552)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:546)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:546)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:524)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:150)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:147)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = oodf\n",
    "# print(df.count())\n",
    "\n",
    "# for col_name in df.columns:\n",
    "#     df = df.withColumn(col_name, \n",
    "#                        when(~isnan(col(col_name)), col(col_name).cast(DoubleType()))\n",
    "#                        .otherwise(np.nan))\n",
    "# df = reconstruct_missing_vals(df)\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "# df = assembler.transform(df)\n",
    "# df.persist()\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(df.count())\n",
    "df.show(3)\n",
    "\n",
    "models = [(\"Gradient Boosted Trees\", GBTRegressor(featuresCol=\"features\", labelCol=\"PRESSURE\",\n",
    "                   maxDepth=5, maxIter=36, stepSize=0.05, subsamplingRate=0.25)), (\"Linear Regression\", LinearRegression(maxIter=100,featuresCol=\"features\", labelCol=\"PRESSURE\"))]\n",
    "# models = [LinearRegression(featuresCol=\"features\", labelCol=\"PRESSURE\")]\n",
    "for name, model in models[1:]:\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "    train_data, test_data = df.randomSplit([0.7, 0.3], seed=17)\n",
    "\n",
    "    predictions = pipeline.fit(train_data).transform(test_data)\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"PRESSURE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test data for {name} = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4baf8372-2ec1-4612-b084-bdd766e71ee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for name, model in models[:1]:\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "\n",
    "    train_data, test_data = df.randomSplit([0.7, 0.3], seed=17)\n",
    "\n",
    "    predictions = pipeline.fit(train_data).transform(test_data)\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"PRESSURE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test data for {name} = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762b1d9a-1634-4d1b-a3e6-e8311c145549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_index(df):\n",
    "    window_spec = Window.orderBy(lit(1))\n",
    "    \n",
    "    df_with_index = df.withColumn(\"index\", row_number().over(window_spec) - 1)\n",
    "    return df_with_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5748e3b5-a036-4bb3-b800-040bc9f8d4f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_diff(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "    \n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "\n",
    "    joined_df = df1.join(df2, on=\"index\", how=\"inner\")\n",
    "\n",
    "    columns = []\n",
    "    for col_name in df1.columns:\n",
    "        if col_name != \"index\": columns.append(col_name)\n",
    "        \n",
    "    sum_of_squared_diff = joined_df.select(\n",
    "        *[\n",
    "            spark_sum(\n",
    "                when(\n",
    "                    isnan(col(f\"{col_name}\")) | isnan(col(f\"{col_name}_2\")),\n",
    "                    0\n",
    "                ).otherwise(pow(col(f\"{col_name}\") - col(f\"{col_name}_2\"), 2))\n",
    "            ).alias(f\"sum_of_squared_{col_name}\")\n",
    "            for col_name in columns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    total_sum_of_squared_diff = sum_of_squared_diff.select(\n",
    "        sum([spark_sum(col_name)\n",
    "        for col_name in sum_of_squared_diff.columns]).alias(\"total_sum_of_squared_diff\")\n",
    "    )\n",
    "\n",
    "    # total_sum_of_squared_diff.show()\n",
    "    return total_sum_of_squared_diff.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eea1914-c957-4d29-bb6b-aa83ee35dc8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|\n+---+---+-----+---------+------------+--------------------+\n|  1|  1|    1|        8|           1|                 1.0|\n|  2|  1|    2|        7|           2|                 2.0|\n|  2|  1|    3|        6|           3|                 3.0|\n|  4|  2|    4|        5|           5|                 7.0|\n|  4|  2|    5|        4|           7|                11.0|\n|  4|  2|    6|        3|           9|                15.0|\n|  5|  8|    7|        2|          17|                79.0|\n|  8|  8|    8|        1|          25|               143.0|\n|  9|  8|    9|        1|          33|               207.0|\n+---+---+-----+---------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 1),\n",
    "    (2, 1),\n",
    "    (2, 1),\n",
    "    (4, 2),\n",
    "    (4, 2),\n",
    "    (4, 2),\n",
    "    (5, 8),\n",
    "    (8, 8),\n",
    "    (9, 8),\n",
    "]\n",
    "columns = [\"A\", \"Y\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "window_spec_index = Window.orderBy(\"A\")\n",
    "df_with_index = df.withColumn(\"index\", row_number().over(window_spec_index))\n",
    "N = df_with_index.count()  \n",
    "df_with_index = df_with_index.withColumn(\n",
    "    \"remaining\",\n",
    "    when((lit(N) - col(\"index\")) == 0, 1).otherwise(lit(N) - col(\"index\"))\n",
    ")\n",
    "\n",
    "df_sorted = df_with_index.orderBy(col(\"A\"))\n",
    "\n",
    "window_spec_prefix = Window.orderBy(\"A\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "# window_spec_suffix = Window.orderBy(\"A\").rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "\n",
    "df_with_sums = df_sorted.withColumn(\"prefix_sum_Y\", spark_sum(col(\"Y\")).over(window_spec_prefix)) \\\n",
    "                        .withColumn(\"prefix_squared_sum_Y\", spark_sum(pow(col(\"Y\"), 2)).over(window_spec_prefix)) \\\n",
    "                        # .withColumn(\"suffix_sum_Y\", spark_sum(col(\"Y\")).over(window_spec_suffix))\n",
    "\n",
    "df_with_sums.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08613a1-d6fc-4fcf-9ca9-a25970f76314",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+-----------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|              RSS|\n+---+---+-----+---------+------------+--------------------+-----------------+\n|  1|  1|    1|        8|           1|                 1.0|             78.0|\n|  2|  1|    2|        7|           2|                 2.0|67.71428571428572|\n|  2|  1|    3|        6|           3|                 3.0|             54.0|\n|  4|  2|    4|        5|           5|                 7.0|43.94999999999999|\n|  4|  2|    5|        4|           7|                11.0|28.19999999999999|\n|  4|  2|    6|        3|           9|                15.0|              1.5|\n|  5|  8|    7|        2|          17|                79.0|37.71428571428572|\n|  8|  8|    8|        1|          25|               143.0|           64.875|\n|  9|  8|    9|        1|          33|               207.0|             86.0|\n+---+---+-----+---------+------------+--------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "tot_prefix_sum_Y = df_with_sums.orderBy(\"prefix_sum_Y\", ascending=False).limit(1).collect()[0][\"prefix_sum_Y\"]\n",
    "\n",
    "tot_prefix_squared_sum_Y = df_with_sums.orderBy(\"prefix_squared_sum_Y\", ascending=False).limit(1).collect()[0][\"prefix_squared_sum_Y\"]\n",
    "\n",
    "# print(tot_prefix_squared_sum_Y)\n",
    "N = df_with_sums.count()  \n",
    "\n",
    "df_final = df_with_sums.withColumn(\n",
    "    \"RSS\",\n",
    "    tot_prefix_squared_sum_Y - \n",
    "    pow(col(\"prefix_sum_Y\"), 2) / col(\"index\") -\n",
    "    pow(tot_prefix_sum_Y-col(\"prefix_sum_Y\"), 2) / col(\"remaining\")\n",
    ")\n",
    "\n",
    "df_final.show()\n",
    "# O(n) rather than O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c00bf94-9c74-4fe5-8fff-b3d8f64bdb13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+-----------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|              RSS|\n+---+---+-----+---------+------------+--------------------+-----------------+\n|  1|  1|    1|        8|           1|                 1.0|             78.0|\n|  2|  1|    2|        7|           2|                 2.0|         Infinity|\n|  2|  1|    3|        6|           3|                 3.0|             54.0|\n|  4|  2|    4|        5|           5|                 7.0|         Infinity|\n|  4|  2|    5|        4|           7|                11.0|         Infinity|\n|  4|  2|    6|        3|           9|                15.0|              1.5|\n|  5|  8|    7|        2|          17|                79.0|37.71428571428572|\n|  8|  8|    8|        1|          25|               143.0|           64.875|\n|  9|  8|    9|        1|          33|               207.0|             86.0|\n+---+---+-----+---------+------------+--------------------+-----------------+\n\n4 1.5\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"index\")\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit, lead\n",
    "\n",
    "df_final = df_final.withColumn(\"lead_A\", lead(\"A\").over(window_spec))\n",
    "df_final = df_final.withColumn(\n",
    "    \"RSS\",\n",
    "    when((col(\"lead_A\").isNotNull()) & (col(\"lead_A\") <= col(\"A\")), lit(float(\"inf\"))).otherwise(col(\"RSS\"))\n",
    ")\n",
    "\n",
    "df_final = df_final.drop(\"lead_A\")\n",
    "df_final.show()\n",
    "df_final_sorted = df_final.orderBy(col(\"RSS\"))\n",
    "\n",
    "row0 = df_final_sorted.limit(1).collect()[0]\n",
    "A, min_RSS = row0[\"A\"], row0[\"RSS\"]\n",
    "print(A, min_RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9c57af-2ccc-44a0-823e-6f9e30563ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_diff(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "    \n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "\n",
    "    joined_df = df1.join(df2, on=\"index\", how=\"inner\")\n",
    "\n",
    "    columns = []\n",
    "    for col_name in numeric_cols:\n",
    "        if col_name != \"index\": columns.append(col_name)\n",
    "        \n",
    "    sum_of_squared_diff = joined_df.select(\n",
    "        *[\n",
    "            spark_sum(\n",
    "                when(\n",
    "                    isnan(col(f\"{col_name}\")) | isnan(col(f\"{col_name}_2\")),\n",
    "                    0\n",
    "                ).otherwise(pow(col(f\"{col_name}\") - col(f\"{col_name}_2\"), 2))\n",
    "            ).alias(f\"sum_of_squared_{col_name}\")\n",
    "            for col_name in columns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    total_sum_of_squared_diff = sum_of_squared_diff.select(\n",
    "        sum([spark_sum(col_name)\n",
    "        for col_name in sum_of_squared_diff.columns]).alias(\"total_sum_of_squared_diff\")\n",
    "    )\n",
    "\n",
    "    # total_sum_of_squared_diff.show()\n",
    "    return total_sum_of_squared_diff.collect()[0][0]\n",
    "    \n",
    "def replace_nans(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "\n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "    joined_df = df1.join(df2, on=\"index\")\n",
    "\n",
    "    for column in numeric_cols:\n",
    "        if column != \"index\": \n",
    "            joined_df = joined_df.withColumn(\n",
    "                column,\n",
    "                when(~isnan(col(f\"{column}\")), col(f\"{column}\")).otherwise(col(f\"{column}_2\"))\n",
    "            )\n",
    "    \n",
    "    cols_to_drop = [col for col in joined_df.columns if col.endswith('_2')] + ['index']\n",
    "    result_df = joined_df.drop(*cols_to_drop)\n",
    "    return result_df\n",
    "\n",
    "def keep_nans(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "\n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "    joined_df = df1.join(df2, on=\"index\")\n",
    "\n",
    "    for column in numeric_cols:\n",
    "        if column != \"index\": \n",
    "            joined_df = joined_df.withColumn(\n",
    "                column,\n",
    "                when(isnan(col(f\"{column}\")), np.nan).otherwise(col(f\"{column}_2\"))\n",
    "            )\n",
    "    \n",
    "    cols_to_drop = [col for col in joined_df.columns if col.endswith('_2')] + ['index']\n",
    "    result_df = joined_df.drop(*cols_to_drop)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb1cc45-f333-4c92-84d5-b5fa637e310d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data = [\n",
    "#     (np.nan, 1.0, 0.0, 0.8414709848078965),\n",
    "#     (2.0, 4.0, 0.6931471805599453, 0.9092974268256817),\n",
    "#     (3.0, 9.0, 1.0986122886681098, np.nan),\n",
    "#     (4.0, np.nan, 1.3862943611198906, -0.7568024953079282),\n",
    "#     (5.0, 25.0, 1.6094379124341003, np.nan),\n",
    "#     (6.0, 36.0, 1.791759469228055, -0.27941549819892586),\n",
    "#     (7.0, np.nan, 1.9459101490553132, 0.6569865987187891),\n",
    "#     (8.0, 64.0, np.nan, 0.9893582466233818),\n",
    "#     (9.0, 81.0, 2.1972245773362196, 0.4121184852417566),\n",
    "#     (10.0, 100.0, np.nan, -0.5440211108893698),\n",
    "#     (11.0, 121.0, 2.3978952727983707, -0.9999902065507035),\n",
    "#     (12.0, 144.0, 2.4849066497880004, -0.5365729180004349),\n",
    "#     (13.0, 169.0, np.nan, 0.4201670368266409),\n",
    "#     (14.0, 196.0, 2.6390573296152584, 0.9906073556948704),\n",
    "#     (np.nan, 225.0, 2.70805020110221, 0.6502878401571168)\n",
    "# ]\n",
    "\n",
    "# columns = [\"a\", \"b\", \"c\", \"d\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "def reconstruct_missing_vals(df):\n",
    "    odf = df # Original DF\n",
    "\n",
    "    columns = numeric_cols\n",
    "    imputer = Imputer(inputCols=columns, outputCols=columns).setStrategy(\"mean\")\n",
    "    df = imputer.fit(df).transform(df)\n",
    "\n",
    "    df = standardize_df(df)\n",
    "    df_standardized_with_nan = keep_nans(odf, df)\n",
    "\n",
    "    # Iterations for feature reconstruction\n",
    "    for i in range(5):\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "        df = assembler.transform(df)\n",
    "\n",
    "        pca = PCA(k=4, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "        model = pca.fit(df)\n",
    "\n",
    "        result = model.transform(df).select(\"features\", \"pcaFeatures\")\n",
    "\n",
    "        pc_matrix = model.pc.toArray().transpose()\n",
    "\n",
    "        # pc_matrix broadcast? Down below\n",
    "        pc_vectors = [Vectors.dense(row) for row in pc_matrix]\n",
    "        # pc_vectors = spark.sparkContext.broadcast(pc_v)\n",
    "        # pc_matrix is small and can easily fit into memory\n",
    "        # print(pc_matrix.shape, pc_matrix)\n",
    "        def reconstruct_features_udf(pca_features):\n",
    "            reconstructed_features = Vectors.dense(\n",
    "                [sum(pca_feature * pc_vector[i] for pca_feature, pc_vector in zip(pca_features, pc_vectors)) for i in range(len(pc_vectors[0]))]\n",
    "            )\n",
    "            return reconstructed_features\n",
    "\n",
    "        reconstruct_features = udf(reconstruct_features_udf, VectorUDT())\n",
    "\n",
    "        reconstructed_df = result.withColumn(\"reconstructed_features\", reconstruct_features(col(\"pcaFeatures\"))).drop(\"pcaFeatures\").drop(\"features\").withColumnRenamed(\"reconstructed_features\", \"features\")\n",
    "\n",
    "        # reconstructed_df.show(truncate=False)\n",
    "        recon_df = reverse_vec_assemble(reconstructed_df, df.columns)\n",
    "        diff = compute_diff(recon_df, df_standardized_with_nan)\n",
    "        # print(diff)\n",
    "        df = replace_nans(df_standardized_with_nan, recon_df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3857e5-07fb-45da-94cf-522f1111641a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def standardize_df(df):\n",
    "    for column in numeric_cols:\n",
    "        mean_val = df.select(mean(col(column))).first()[0]\n",
    "        stddev_val = df.select(stddev(col(column))).first()[0]\n",
    "        df = df.withColumn(column, (col(column) - mean_val) / stddev_val)\n",
    "    return df\n",
    "\n",
    "# data = [(1, 2), (2, 4), (3, 6)]\n",
    "# columns = [\"col1\", \"col2\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "# df = standardize_df(df)\n",
    "# df.show()\n",
    "\n",
    "def reverse_vec_assemble(df_assembled, columns):\n",
    "    vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "    df_array = df_assembled.withColumn(\"features_array\", vector_to_array(\"features\"))\n",
    "\n",
    "    for i, col_name in enumerate(columns):\n",
    "        df_array = df_array.withColumn(col_name, df_array[\"features_array\"].getItem(i))\n",
    "\n",
    "    df_array = df_array.drop(\"features_array\").drop(\"features\")\n",
    "    return df_array"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "air_ml",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
