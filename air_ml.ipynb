{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82dffa15-72c7-430f-b2ae-97f48c255546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf, col, split\n",
    "from pyspark.ml.feature import Imputer, PCA\n",
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit, lead, isnan\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69dd3d04-6bf2-4460-b0fa-3a7c224a0e80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_directory_path = \"dbfs:/FileStore/tables\"\n",
    "files = spark.sparkContext.wholeTextFiles(csv_directory_path).keys().collect()\n",
    "csv_files = [file for file in files if file.endswith(\".csv\")]\n",
    "# for file_name in csv_files:\n",
    "#     dbutils.fs.rm(file_name)\n",
    "# %fs rm dbfs:/FileStore/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d02df6-eafb-4508-96c3-6091008eae10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files in the directory: 300\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"AirML\").getOrCreate()\n",
    "\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"Number of CSV files in the directory: {num_csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84efa060-3b7e-4e4b-b5fa-bb12a5ba4fae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000\n"
     ]
    }
   ],
   "source": [
    "# print(csv_files)\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_files)\n",
    "    # .option(\"mergeSchema\", \"true\") \\\n",
    "# print(df.count())\n",
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND\", \"CIG\", \"VIS\", \"TMP\", \"DEW\", \"SLP\")\n",
    "sampled_rows = df.rdd.takeSample(False, 45000, seed=177)\n",
    "df = spark.createDataFrame(sampled_rows, df.schema)\n",
    "df.persist()\n",
    "print(df.count())\n",
    "# df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a3d337f-4d4d-4782-bd07-346670ab4109",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring_index, split, col\n",
    "\n",
    "df = df.withColumn(\"WND_DIR\", split(col(\"WND\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"WND_TYPE\", split(col(\"WND\"), \",\").getItem(2))\n",
    "df = df.withColumn(\"WND_SPD\", split(col(\"WND\"), \",\").getItem(3))\n",
    "df = df.withColumn(\"CEL_H\", split(col(\"CIG\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"VIS_DIST\", split(col(\"VIS\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"VIS_VAR\", split(col(\"VIS\"), \",\").getItem(2))\n",
    "df = df.withColumn(\"AIR_TMP\", split(col(\"TMP\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"DEW_TMP\", split(col(\"DEW\"), \",\").getItem(0))\n",
    "df = df.withColumn(\"PRESSURE\", split(col(\"SLP\"), \",\").getItem(0))\n",
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_TYPE\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"VIS_VAR\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")\n",
    "# df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723ccb84-889f-42e0-b8ec-2bbb86e076ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"AIR_TMP\", \"DEW_TMP\"]\n",
    "# categorical_cols = []\n",
    "categorical_cols = [\"WND_TYPE\", \"VIS_VAR\"]\n",
    "all_cols = numeric_cols + categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d39b969d-4b5a-4d1d-8eb8-0409833cfe26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ed527cef704ae7bdd87a736b5c9168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3088f9d9810459f8434626e1b190726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do not execute the above code unless you want to deal with categorical vars\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\") for col in categorical_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_ONE\") for col in categorical_cols]\n",
    "\n",
    "pipeline_stages = indexers + encoders \n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.drop(\"WND_TYPE\")\n",
    "df = df.drop(\"VIS_VAR\")\n",
    "df = df.drop(\"WND_TYPE_Index\")\n",
    "df = df.drop(\"VIS_VAR_Index\")\n",
    "df = df.withColumnRenamed(\"VIS_VAR_ONE\", \"VIS_VAR\").withColumnRenamed(\"WND_TYPE_ONE\", \"WND_TYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f34e829-5c22-40e2-90ee-f839605ef8cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+-------------+-------+-----+--------+-------------+-------+-------+--------+\n|LONGITUDE|LATITUDE|ELEVATION|WND_DIR|     WND_TYPE|WND_SPD|CEL_H|VIS_DIST|      VIS_VAR|AIR_TMP|DEW_TMP|PRESSURE|\n+---------+--------+---------+-------+-------------+-------+-----+--------+-------------+-------+-------+--------+\n|-78.93818| 43.1083|    178.3|    110|(5,[0],[1.0])|   0015|00762|  016093|(2,[1],[1.0])|  +0228|  +0206|   99999|\n|-104.7552| 40.8066|   1642.9|    999|(5,[1],[1.0])|   9999|99999|  999999|(2,[0],[1.0])|  +0041|  +9999|   99999|\n+---------+--------+---------+-------+-------------+-------+-----+--------+-------------+-------+-------+--------+\nonly showing top 2 rows\n\n45000\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_TYPE\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"VIS_VAR\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")\n",
    "# df = df.select(\"LONGITUDE\", \"LATITUDE\", \"ELEVATION\", \"WND_DIR\", \"WND_SPD\", \"CEL_H\", \"VIS_DIST\", \"AIR_TMP\", \"DEW_TMP\", \"PRESSURE\")\n",
    "df.show(2)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f444e021-c30e-45e5-bd5a-d60b8bd03340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17961\n+-----------+----------+---------+-------+-------------+-------+-----+--------+-------------+-------+-------+--------+\n|  LONGITUDE|  LATITUDE|ELEVATION|WND_DIR|     WND_TYPE|WND_SPD|CEL_H|VIS_DIST|      VIS_VAR|AIR_TMP|DEW_TMP|PRESSURE|\n+-----------+----------+---------+-------+-------------+-------+-----+--------+-------------+-------+-------+--------+\n|-36.6166666|-9.4166666|    276.5|    360|(5,[0],[1.0])|   0018|  NaN|     NaN|(2,[0],[1.0])|  +0222|    NaN|   10176|\n|158.2166667| 6.9666667|     39.0|    120|(5,[0],[1.0])|   0015|09144|  024140|(2,[0],[1.0])|  +0232|  +0229|   10113|\n| 46.5833333|56.3333333|    107.0|    200|(5,[0],[1.0])|   0010|22000|  010000|(2,[0],[1.0])|  +0203|  +0159|   10124|\n+-----------+----------+---------+-------+-------------+-------+-----+--------+-------------+-------+-------+--------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Cannot predict if the response variable is missing\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\"LONGITUDE\", when(col(\"LONGITUDE\") == 999999, np.nan).otherwise(col(\"LONGITUDE\"))) \\\n",
    "       .withColumn(\"LATITUDE\", when(col(\"LATITUDE\") == 99999, np.nan).otherwise(col(\"LATITUDE\"))) \\\n",
    "       .withColumn(\"ELEVATION\", when(col(\"ELEVATION\") == 9999, np.nan).otherwise(col(\"ELEVATION\"))) \\\n",
    "       .withColumn(\"WND_DIR\", when(col(\"WND_DIR\") == 999, np.nan).otherwise(col(\"WND_DIR\"))) \\\n",
    "       .withColumn(\"WND_SPD\", when(col(\"WND_SPD\") == 9999, np.nan).otherwise(col(\"WND_SPD\"))) \\\n",
    "       .withColumn(\"CEL_H\", when(col(\"CEL_H\") == 99999, np.nan).otherwise(col(\"CEL_H\"))) \\\n",
    "       .withColumn(\"VIS_DIST\", when(col(\"VIS_DIST\") == 999999, np.nan).otherwise(col(\"VIS_DIST\"))) \\\n",
    "       .withColumn(\"AIR_TMP\", when(col(\"AIR_TMP\") == 9999, np.nan).otherwise(col(\"AIR_TMP\"))) \\\n",
    "       .withColumn(\"DEW_TMP\", when(col(\"DEW_TMP\") == 9999, np.nan).otherwise(col(\"DEW_TMP\")))\n",
    "\n",
    "df = df.filter(df.PRESSURE != 99999)\n",
    "\n",
    "print(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de5e464-0fbb-42aa-b64f-18cfa2bccdc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[LONGITUDE: string, LATITUDE: string, ELEVATION: string, WND_DIR: string, WND_TYPE: vector, WND_SPD: string, CEL_H: string, VIS_DIST: string, VIS_VAR: vector, AIR_TMP: string, DEW_TMP: string, PRESSURE: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.persist()\n",
    "oodf = df\n",
    "oodf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9f85ce-f4fd-49f8-9fd0-b719c8e7fade",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# assembler_numeric = VectorAssembler(inputCols=numeric_cols, outputCol=\"numerical_features\")\n",
    "# assembler_all = VectorAssembler(inputCols=[\"scaled_numerical_features\"] + [\"WND_TYPE\", \"VIS_VAR\"], outputCol=\"features\")\n",
    "# pipeline = Pipeline(stages=[assembler_numeric, assembler_all])\n",
    "# df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a6c56c-e4a6-4a8c-8141-8fded02efd3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17961\n+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------+-------------+--------+--------------------+\n|          LONGITUDE|           LATITUDE|           ELEVATION|            WND_DIR|            WND_SPD|              CEL_H|           VIS_DIST|           AIR_TMP|            DEW_TMP|     WND_TYPE|      VIS_VAR|PRESSURE|            features|\n+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------+-------------+--------+--------------------+\n|-0.3499966248135452| -1.146269281536541|-0.02330227515780...| 1.8167372340807275|-0.5581181642885598| 1.6315420631295205|  1.132230272540409|0.7089061816297165|0.09084793007516523|(5,[0],[1.0])|(2,[0],[1.0])| 10176.0|[-0.3499966248135...|\n| 1.7795548730154005|-0.6469847598094987| -0.5567363386968415|-0.7165026958757874|-0.6652703506166985|-0.4017705339083754|  0.661300845732293|0.7865924050161338|  1.352047620855845|(5,[0],[1.0])|(2,[0],[1.0])| 10113.0|[1.77955487301540...|\n| 0.5593892662218319| 0.8574717553148902| -0.4040057436625066|0.12791061410971757|-0.8438573278302631| 1.5714997395219807|-0.8031185077792483|0.5613023571955237| 0.7579026610891655|(5,[0],[1.0])|(2,[0],[1.0])| 10124.0|[0.55938926622183...|\n+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+-------------+-------------+--------+--------------------+\nonly showing top 3 rows\n\n17961\n"
     ]
    }
   ],
   "source": [
    "print(oodf.count())\n",
    "\n",
    "num_df = oodf.select(*numeric_cols, \"PRESSURE\")\n",
    "cat_df = oodf.select(*categorical_cols)\n",
    "\n",
    "for col_name in numeric_cols + [\"PRESSURE\"]:\n",
    "    num_df = num_df.withColumn(col_name, \n",
    "                       when(~isnan(col(col_name)), col(col_name).cast(DoubleType()))\n",
    "                       .otherwise(np.nan))\n",
    "num_df = reconstruct_missing_vals(num_df)\n",
    "\n",
    "num_df = add_index(num_df)\n",
    "cat_df = add_index(cat_df)\n",
    "joined_df = num_df.join(cat_df, on=\"index\")\n",
    "df = joined_df.select(*all_cols, \"PRESSURE\")\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "# df.persist(StorageLevel.MEMORY_ONLY)\n",
    "df.persist()\n",
    "df.show(3)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bab928c-ebf3-40dc-b892-7b70a2ffa02d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd7bfc3a7e74e0aad0e9fab832d035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4dbe576ec04fba9b80d1634bc5d4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Linear Regression = 89.39472338690288\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33667d1582b433492032048e844794d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880099afe8f844ea84db4e706520c50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data for Gradient Boosted Trees = 78.80678487809404\n"
     ]
    }
   ],
   "source": [
    "models = [(\"Linear Regression\", LinearRegression(featuresCol=\"features\", labelCol=\"PRESSURE\")), (\"Gradient Boosted Trees\", GBTRegressor(featuresCol=\"features\", labelCol=\"PRESSURE\",\n",
    "                   maxDepth=7, maxIter=100, stepSize=0.1, subsamplingRate=0.75))]\n",
    "# models = [LinearRegression(featuresCol=\"features\", labelCol=\"PRESSURE\")]\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline(stages=[model])\n",
    "\n",
    "    train_data, test_data = df.randomSplit([0.7, 0.3], seed=1777)\n",
    "\n",
    "    predictions = pipeline.fit(train_data).transform(test_data)\n",
    "\n",
    "    evaluator = RegressionEvaluator(labelCol=\"PRESSURE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(f\"Root Mean Squared Error (RMSE) on test data for {name} = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762b1d9a-1634-4d1b-a3e6-e8311c145549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_index(df):\n",
    "    window_spec = Window.orderBy(lit(1))\n",
    "    \n",
    "    df_with_index = df.withColumn(\"index\", row_number().over(window_spec) - 1)\n",
    "    return df_with_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eea1914-c957-4d29-bb6b-aa83ee35dc8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|\n+---+---+-----+---------+------------+--------------------+\n|  1|  1|    1|        8|           1|                 1.0|\n|  2|  1|    2|        7|           2|                 2.0|\n|  2|  1|    3|        6|           3|                 3.0|\n|  4|  2|    4|        5|           5|                 7.0|\n|  4|  2|    5|        4|           7|                11.0|\n|  4|  2|    6|        3|           9|                15.0|\n|  5|  8|    7|        2|          17|                79.0|\n|  8|  8|    8|        1|          25|               143.0|\n|  9|  8|    9|        1|          33|               207.0|\n+---+---+-----+---------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 1),\n",
    "    (2, 1),\n",
    "    (2, 1),\n",
    "    (4, 2),\n",
    "    (4, 2),\n",
    "    (4, 2),\n",
    "    (5, 8),\n",
    "    (8, 8),\n",
    "    (9, 8),\n",
    "]\n",
    "columns = [\"A\", \"Y\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "window_spec_index = Window.orderBy(\"A\")\n",
    "df_with_index = df.withColumn(\"index\", row_number().over(window_spec_index))\n",
    "N = df_with_index.count()  \n",
    "df_with_index = df_with_index.withColumn(\n",
    "    \"remaining\",\n",
    "    when((lit(N) - col(\"index\")) == 0, 1).otherwise(lit(N) - col(\"index\"))\n",
    ")\n",
    "\n",
    "df_sorted = df_with_index.orderBy(col(\"A\"))\n",
    "\n",
    "window_spec_prefix = Window.orderBy(\"A\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "# window_spec_suffix = Window.orderBy(\"A\").rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "\n",
    "df_with_sums = df_sorted.withColumn(\"prefix_sum_Y\", spark_sum(col(\"Y\")).over(window_spec_prefix)) \\\n",
    "                        .withColumn(\"prefix_squared_sum_Y\", spark_sum(pow(col(\"Y\"), 2)).over(window_spec_prefix)) \\\n",
    "                        # .withColumn(\"suffix_sum_Y\", spark_sum(col(\"Y\")).over(window_spec_suffix))\n",
    "\n",
    "df_with_sums.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08613a1-d6fc-4fcf-9ca9-a25970f76314",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+-----------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|              RSS|\n+---+---+-----+---------+------------+--------------------+-----------------+\n|  1|  1|    1|        8|           1|                 1.0|             78.0|\n|  2|  1|    2|        7|           2|                 2.0|67.71428571428572|\n|  2|  1|    3|        6|           3|                 3.0|             54.0|\n|  4|  2|    4|        5|           5|                 7.0|43.94999999999999|\n|  4|  2|    5|        4|           7|                11.0|28.19999999999999|\n|  4|  2|    6|        3|           9|                15.0|              1.5|\n|  5|  8|    7|        2|          17|                79.0|37.71428571428572|\n|  8|  8|    8|        1|          25|               143.0|           64.875|\n|  9|  8|    9|        1|          33|               207.0|             86.0|\n+---+---+-----+---------+------------+--------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "tot_prefix_sum_Y = df_with_sums.orderBy(\"prefix_sum_Y\", ascending=False).limit(1).collect()[0][\"prefix_sum_Y\"]\n",
    "\n",
    "tot_prefix_squared_sum_Y = df_with_sums.orderBy(\"prefix_squared_sum_Y\", ascending=False).limit(1).collect()[0][\"prefix_squared_sum_Y\"]\n",
    "\n",
    "# print(tot_prefix_squared_sum_Y)\n",
    "N = df_with_sums.count()  \n",
    "\n",
    "df_final = df_with_sums.withColumn(\n",
    "    \"RSS\",\n",
    "    tot_prefix_squared_sum_Y - \n",
    "    pow(col(\"prefix_sum_Y\"), 2) / col(\"index\") -\n",
    "    pow(tot_prefix_sum_Y-col(\"prefix_sum_Y\"), 2) / col(\"remaining\")\n",
    ")\n",
    "\n",
    "df_final.show()\n",
    "# O(n) rather than O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c00bf94-9c74-4fe5-8fff-b3d8f64bdb13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---------+------------+--------------------+-----------------+\n|  A|  Y|index|remaining|prefix_sum_Y|prefix_squared_sum_Y|              RSS|\n+---+---+-----+---------+------------+--------------------+-----------------+\n|  1|  1|    1|        8|           1|                 1.0|             78.0|\n|  2|  1|    2|        7|           2|                 2.0|         Infinity|\n|  2|  1|    3|        6|           3|                 3.0|             54.0|\n|  4|  2|    4|        5|           5|                 7.0|         Infinity|\n|  4|  2|    5|        4|           7|                11.0|         Infinity|\n|  4|  2|    6|        3|           9|                15.0|              1.5|\n|  5|  8|    7|        2|          17|                79.0|37.71428571428572|\n|  8|  8|    8|        1|          25|               143.0|           64.875|\n|  9|  8|    9|        1|          33|               207.0|             86.0|\n+---+---+-----+---------+------------+--------------------+-----------------+\n\n4 1.5\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(\"index\")\n",
    "from pyspark.sql.functions import col, sum as spark_sum, pow, row_number, monotonically_increasing_id, when, lit, lead\n",
    "\n",
    "df_final = df_final.withColumn(\"lead_A\", lead(\"A\").over(window_spec))\n",
    "df_final = df_final.withColumn(\n",
    "    \"RSS\",\n",
    "    when((col(\"lead_A\").isNotNull()) & (col(\"lead_A\") <= col(\"A\")), lit(float(\"inf\"))).otherwise(col(\"RSS\"))\n",
    ")\n",
    "\n",
    "df_final = df_final.drop(\"lead_A\")\n",
    "df_final.show()\n",
    "df_final_sorted = df_final.orderBy(col(\"RSS\"))\n",
    "\n",
    "row0 = df_final_sorted.limit(1).collect()[0]\n",
    "A, min_RSS = row0[\"A\"], row0[\"RSS\"]\n",
    "print(A, min_RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9c57af-2ccc-44a0-823e-6f9e30563ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_diff(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "    \n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "\n",
    "    joined_df = df1.join(df2, on=\"index\", how=\"inner\")\n",
    "\n",
    "    columns = []\n",
    "    for col_name in numeric_cols:\n",
    "        if col_name != \"index\": columns.append(col_name)\n",
    "        \n",
    "    sum_of_squared_diff = joined_df.select(\n",
    "        *[\n",
    "            spark_sum(\n",
    "                when(\n",
    "                    isnan(col(f\"{col_name}\")) | isnan(col(f\"{col_name}_2\")),\n",
    "                    0\n",
    "                ).otherwise(pow(col(f\"{col_name}\") - col(f\"{col_name}_2\"), 2))\n",
    "            ).alias(f\"sum_of_squared_{col_name}\")\n",
    "            for col_name in columns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    total_sum_of_squared_diff = sum_of_squared_diff.select(\n",
    "        sum([spark_sum(col_name)\n",
    "        for col_name in sum_of_squared_diff.columns]).alias(\"total_sum_of_squared_diff\")\n",
    "    )\n",
    "\n",
    "    # total_sum_of_squared_diff.show()\n",
    "    return total_sum_of_squared_diff.collect()[0][0]\n",
    "    \n",
    "def replace_nans(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "\n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "    joined_df = df1.join(df2, on=\"index\")\n",
    "\n",
    "    for column in numeric_cols:\n",
    "        if column != \"index\": \n",
    "            joined_df = joined_df.withColumn(\n",
    "                column,\n",
    "                when(~isnan(col(f\"{column}\")), col(f\"{column}\")).otherwise(col(f\"{column}_2\"))\n",
    "            )\n",
    "    \n",
    "    cols_to_drop = [col for col in joined_df.columns if col.endswith('_2')] + ['index']\n",
    "    result_df = joined_df.drop(*cols_to_drop)\n",
    "    return result_df\n",
    "\n",
    "def keep_nans(df1, df2):\n",
    "    for col_name in df2.columns:\n",
    "        df2 = df2.withColumnRenamed(col_name, f\"{col_name}_2\")\n",
    "\n",
    "    df1 = add_index(df1)\n",
    "    df2 = add_index(df2)\n",
    "    joined_df = df1.join(df2, on=\"index\")\n",
    "\n",
    "    for column in numeric_cols:\n",
    "        if column != \"index\": \n",
    "            joined_df = joined_df.withColumn(\n",
    "                column,\n",
    "                when(isnan(col(f\"{column}\")), np.nan).otherwise(col(f\"{column}_2\"))\n",
    "            )\n",
    "    \n",
    "    cols_to_drop = [col for col in joined_df.columns if col.endswith('_2')] + ['index']\n",
    "    result_df = joined_df.drop(*cols_to_drop)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb1cc45-f333-4c92-84d5-b5fa637e310d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data = [\n",
    "#     (np.nan, 1.0, 0.0, 0.8414709848078965),\n",
    "#     (2.0, 4.0, 0.6931471805599453, 0.9092974268256817),\n",
    "#     (3.0, 9.0, 1.0986122886681098, np.nan),\n",
    "#     (4.0, np.nan, 1.3862943611198906, -0.7568024953079282),\n",
    "#     (5.0, 25.0, 1.6094379124341003, np.nan),\n",
    "#     (6.0, 36.0, 1.791759469228055, -0.27941549819892586),\n",
    "#     (7.0, np.nan, 1.9459101490553132, 0.6569865987187891),\n",
    "#     (8.0, 64.0, np.nan, 0.9893582466233818),\n",
    "#     (9.0, 81.0, 2.1972245773362196, 0.4121184852417566),\n",
    "#     (10.0, 100.0, np.nan, -0.5440211108893698),\n",
    "#     (11.0, 121.0, 2.3978952727983707, -0.9999902065507035),\n",
    "#     (12.0, 144.0, 2.4849066497880004, -0.5365729180004349),\n",
    "#     (13.0, 169.0, np.nan, 0.4201670368266409),\n",
    "#     (14.0, 196.0, 2.6390573296152584, 0.9906073556948704),\n",
    "#     (np.nan, 225.0, 2.70805020110221, 0.6502878401571168)\n",
    "# ]\n",
    "\n",
    "# columns = [\"a\", \"b\", \"c\", \"d\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "def reconstruct_missing_vals(df):\n",
    "    odf = df # Original DF\n",
    "\n",
    "    columns = numeric_cols\n",
    "    imputer = Imputer(inputCols=columns, outputCols=columns).setStrategy(\"mean\")\n",
    "    df = imputer.fit(df).transform(df)\n",
    "\n",
    "    df = standardize_df(df)\n",
    "    df_standardized_with_nan = keep_nans(odf, df)\n",
    "\n",
    "    # Iterations for feature reconstruction\n",
    "    for i in range(7):\n",
    "        assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "        df = assembler.transform(df)\n",
    "\n",
    "        pca = PCA(k=4, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "        model = pca.fit(df)\n",
    "\n",
    "        result = model.transform(df).select(\"features\", \"pcaFeatures\")\n",
    "\n",
    "        pc_matrix = model.pc.toArray().transpose()\n",
    "\n",
    "        # pc_matrix broadcast? Down below\n",
    "        pc_vectors = [Vectors.dense(row) for row in pc_matrix]\n",
    "        # pc_vectors = spark.sparkContext.broadcast(pc_v)\n",
    "        # pc_matrix is small and can easily fit into memory\n",
    "        # print(pc_matrix.shape, pc_matrix)\n",
    "        def reconstruct_features_udf(pca_features):\n",
    "            reconstructed_features = Vectors.dense(\n",
    "                [sum(pca_feature * pc_vector[i] for pca_feature, pc_vector in zip(pca_features, pc_vectors)) for i in range(len(pc_vectors[0]))]\n",
    "            )\n",
    "            return reconstructed_features\n",
    "\n",
    "        reconstruct_features = udf(reconstruct_features_udf, VectorUDT())\n",
    "\n",
    "        reconstructed_df = result.withColumn(\"reconstructed_features\", reconstruct_features(col(\"pcaFeatures\"))).drop(\"pcaFeatures\").drop(\"features\").withColumnRenamed(\"reconstructed_features\", \"features\")\n",
    "\n",
    "        # reconstructed_df.show(truncate=False)\n",
    "        recon_df = reverse_vec_assemble(reconstructed_df, df.columns)\n",
    "        diff = compute_diff(recon_df, df_standardized_with_nan)\n",
    "        # print(diff)\n",
    "        df = replace_nans(df_standardized_with_nan, recon_df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc3857e5-07fb-45da-94cf-522f1111641a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def standardize_df(df):\n",
    "    for column in numeric_cols:\n",
    "        mean_val = df.select(mean(col(column))).first()[0]\n",
    "        stddev_val = df.select(stddev(col(column))).first()[0]\n",
    "        df = df.withColumn(column, (col(column) - mean_val) / stddev_val)\n",
    "    return df\n",
    "\n",
    "# data = [(1, 2), (2, 4), (3, 6)]\n",
    "# columns = [\"col1\", \"col2\"]\n",
    "# df = spark.createDataFrame(data, columns)\n",
    "# df = standardize_df(df)\n",
    "# df.show()\n",
    "\n",
    "def reverse_vec_assemble(df_assembled, columns):\n",
    "    vector_to_array = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "    df_array = df_assembled.withColumn(\"features_array\", vector_to_array(\"features\"))\n",
    "\n",
    "    for i, col_name in enumerate(columns):\n",
    "        df_array = df_array.withColumn(col_name, df_array[\"features_array\"].getItem(i))\n",
    "\n",
    "    df_array = df_array.drop(\"features_array\").drop(\"features\")\n",
    "    return df_array"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "air_ml",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
